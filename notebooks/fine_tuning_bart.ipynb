{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"fine_tuning_bart.ipynb","provenance":[{"file_id":"https://github.com/tensorflow/models/blob/master/official/colab/fine_tuning_bert.ipynb","timestamp":1606696483960}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"z69h4UZ2l9sT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615542125806,"user_tz":480,"elapsed":376,"user":{"displayName":"紀柏維","photoUrl":"","userId":"11920202532871739533"}},"outputId":"07b98ea1-138c-46ed-806c-fbbf64f55000"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/News_Classification"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/News_Classification\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2B867h8ClrAJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615542133007,"user_tz":480,"elapsed":7549,"user":{"displayName":"紀柏維","photoUrl":"","userId":"11920202532871739533"}},"outputId":"d58b7f88-59ca-4a19-e0ae-a027afce336c"},"source":["!pip install torch\n","!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FkdYUTcplvEz"},"source":["import os\n","import time\n","import json\n","import logging\n","\n","import torch\n","from torch import nn \n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from transformers import pipeline\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IOyT4TR26lBN"},"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")\n","else:\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQvlUpxQyLp3"},"source":["epochs = 50\n","batch_size = 1\n","learning_rate = 1e-5\n","max_token_len = 1024"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JdN0Nt9oxeCY"},"source":["checkpoint_path = os.path.join(os.getcwd(), \"checkpoints\")\n","dataset_path = os.path.join(os.getcwd(), 'dataset')\n","train_data_path = os.path.join(dataset_path, 'processed_train.json')\n","test_data_path = os.path.join(dataset_path, 'processed_test.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8w6-mS2mGIHO"},"source":["# Reference from: https://github.com/yinwenpeng/BenchmarkingZeroShot/blob/master/src/train_yahoo.py\n","\n","choice_to_hypothesis = {\n","    \"Containment and Closure Policies\": [\n","        'it is related with containment and closure policy from governments in the pandemic',\n","        'this news is talking about a government coronavirus policy for containment and closure',\n","        'this text describes a government policy about school closing, workspace closing, public event cancellation, restrictions on gatherings, public transport closing, stay at home requirement, restrictions on internel movement, and international travel control in the pandemic.'\n","    ],\n","    \"Economic Policies\": [\n","        'it is related with economic policy from governments in the pandemic', \n","        'this news is talking about a government coronavirus policy for economy', \n","        'this text describes a government policy about income support, debt or contract relief, fiscal measurements, and international support in the pandemic.'\n","    ],\n","    \"Health System Policies\": [\n","        'it is related with health system policy from governments in the pandemic', \n","        'this news is talking about a government coronavirus policy for health system', \n","        'this text describes a government policy about public health compaigns, testing policy, contact tracing, emergency investment in health care, investment in vaccines, facial coverings, and vaccination policy in the pandemic.'\n","    ],\n","    \"Miscellaneous Policies\": [\n","        'it is related with miscellaneous policy from governments in the pandemic', \n","        'this news is talking about a government policy irrelevant to coronavirus', \n","        'this text describes a government policy that do not fit anywhere else in the pandemic.'\n","    ]\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NBhpIlDucPUq"},"source":["# Load the pretrained Model"]},{"cell_type":"code","metadata":{"id":"hWGq74ztp4r2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615542160025,"user_tz":480,"elapsed":34507,"user":{"displayName":"紀柏維","photoUrl":"","userId":"11920202532871739533"}},"outputId":"90978b86-9b9f-40b3-cce0-7dd5559b589d"},"source":["classifier = pipeline(\n","    \"zero-shot-classification\",\n","    model=\"facebook/bart-large-mnli\",\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['model.encoder.version', 'model.decoder.version']\n","- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n","- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"6ZB-HLIwv62f"},"source":["# Prepare Data"]},{"cell_type":"code","metadata":{"id":"Y7mtfVd6MuXu"},"source":["class CovidNewsData(object):\n","    def __init__(self, id, premise=None, hypothesis=None, label=None):\n","        self.id = id\n","        self.premise = premise\n","        self.hypothesis = hypothesis\n","        self.label = label\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PQctKU6h8mO_"},"source":["class CovidNewsDataset(Dataset):\n","    \"\"\"\n","    Reference from https://huggingface.co/transformers/_modules/transformers/pipelines/zero_shot_classification.html#ZeroShotClassificationPipeline.__call__\n","    \"\"\"\n","    def __init__(\n","        self, \n","        data_path=None, \n","        *,\n","        tokenizer=None, \n","        choice_to_hypothesis=None,\n","        target_text='summary', \n","        max_token_len=1024, \n","        device=\"cpu\", \n","        transform=None,\n","      ):\n","        \"\"\"\n","        Args: \n","            data_path (str): The full path of the dataset. Required.\n","            target_text (str): Either use 'summary' or 'article' as inputs of the model. Default: 'summary'.\n","            tokenizer: The model's tokenizer. Required.\n","        \"\"\"\n","\n","        assert data_path is not None, f\"[self.__class__.__name__] Please specify a data path.\"\n","        assert tokenizer is not None, f\"[self.__class__.__name__] Please give a tokenizer.\"\n","        assert isinstance(choice_to_hypothesis, dict), f\"[self.__class__.__name__] Please give a dictionary for choices to hypothesis.\"\n","        assert target_text in ['summary', 'article'], f\"[self.__class__.__name__] Please pick a target_text from either 'summary' or 'article.\"\n","        self.data_path = data_path\n","        self.target_text = target_text\n","        self.tokenizer = tokenizer\n","        self.choice_to_hypothesis = choice_to_hypothesis\n","        self.class_to_id = {\n","            \"Contradiction\": 0, \n","            \"Neutral\": 1, \n","            \"Entailment\": 2, \n","        }\n","        self.max_token_len = max_token_len if max_token_len < tokenizer.model_max_length else tokenizer.model_max_length\n","        self.device = device\n","        self.transform = transform\n","\n","        # Init data\n","        self.data_list = self._get_data()\n","\n","    def __len__(self):\n","        return len(self.data_list)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.item()\n","\n","        data = self.data_list[idx]\n","        encoding = self.tokenizer(\n","            self._get_sentence_pair(data), \n","            return_tensors='pt', \n","            padding='max_length',\n","            add_special_tokens=True, \n","            truncation='only_first',  # prevent from truncating the hypothesis\n","            max_length=self.max_token_len\n","        )\n","        encoding['input_ids'] = encoding['input_ids'].squeeze()\n","        encoding['attention_mask'] = encoding['attention_mask'].squeeze()\n","\n","        sample = {\n","            'id': data.id,\n","            'inputs': self._ensure_tensor_on_device(**encoding), \n","            'label': torch.tensor(self.class_to_id[data.label], dtype=torch.long).to(device)\n","        }\n","\n","        if self.transform is not None:\n","            sample = self.transform(sample)\n","\n","        return sample\n","\n","    def _get_sentence_pair(self, data):\n","\n","        return [[data.premise, data.hypothesis]]\n","\n","    def _ensure_tensor_on_device(self, **inputs):\n","\n","        return {name: tensor.to(self.device) for name, tensor in inputs.items()}\n","\n","    def _get_data(self):\n","        with open(self.data_path) as f:\n","            raw_data_list = json.load(f)\n","\n","        labels = list(self.class_to_id.keys())\n","        data_list = []\n","        for data in raw_data_list:\n","            current_choice = data['choice']\n","            id = data['id']\n","            premise = data[self.target_text]\n","\n","            # True, 3 from the current choice\n","            label = labels[2]   # Entailment\n","            for hypothesis in self.choice_to_hypothesis[current_choice]:\n","                data_list.append(CovidNewsData(\n","                    id=id,\n","                    premise=premise,\n","                    hypothesis=hypothesis,\n","                    label=label,\n","                ))\n","\n","            # False, 3 from other choices seperately\n","            label = labels[0]   # Contradiction or Not Entailment\n","            for other_choice in self.choice_to_hypothesis.keys():\n","                if current_choice == other_choice: continue\n","\n","                randIdx = torch.randperm(3)[0].item()\n","                hypothesis = self.choice_to_hypothesis[other_choice][randIdx]\n","                data_list.append(CovidNewsData(\n","                    id=id,\n","                    premise=premise,\n","                    hypothesis=hypothesis,\n","                    label=label,\n","                ))\n","\n","        return data_list\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EiTTPTxuzHls"},"source":["train_dataset = CovidNewsDataset(\n","    train_data_path,\n","    tokenizer=classifier.tokenizer,\n","    choice_to_hypothesis=choice_to_hypothesis,\n","    device=device\n",")\n","train_data_loader = DataLoader(\n","    dataset=train_dataset, \n","    batch_size=batch_size,\n","    shuffle=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uX1IndkAcpoP"},"source":["test_dataset = CovidNewsDataset(\n","    test_data_path, \n","    tokenizer=classifier.tokenizer, \n","    choice_to_hypothesis=choice_to_hypothesis,\n","    device=device\n",")\n","test_data_loader = DataLoader(\n","    dataset=test_dataset, \n","    batch_size=batch_size,\n","    shuffle=False\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olYXwUMOaZMR","executionInfo":{"status":"ok","timestamp":1615542160528,"user_tz":480,"elapsed":34959,"user":{"displayName":"紀柏維","photoUrl":"","userId":"11920202532871739533"}},"outputId":"ca79dbd7-d731-407d-9180-ea141df8e9a9"},"source":["print(f\"Length of the training dataset: {len(train_dataset)}\")\n","print(f\"Length of the testing dataset: {len(test_dataset)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Length of the training dataset: 720\n","Length of the testing dataset: 2616\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UPVkUa9cc7Lk"},"source":["# Prepare for training"]},{"cell_type":"code","metadata":{"id":"TyNglmki36G4"},"source":["log_interval = 32  # unit: step\n","num_steps_per_epoch = len(train_dataset)//batch_size + 1\n","store_interval = num_steps_per_epoch//2 + 1\n","num_train_steps = epochs*num_steps_per_epoch\n","num_warmup_steps = 1*num_steps_per_epoch  # 1 epoch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"faShGXClvxDZ"},"source":["no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in classifier.model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in classifier.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qKlPhOkKdTIn"},"source":["scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWBQNfXc3gpG"},"source":["criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JFBFYg3U2nH"},"source":["writer = SummaryWriter()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0bA1j5TCoSbM","executionInfo":{"status":"ok","timestamp":1615542163986,"user_tz":480,"elapsed":38362,"user":{"displayName":"紀柏維","photoUrl":"","userId":"11920202532871739533"}},"outputId":"689b928a-91e0-4aa3-9596-09be67ef51ec"},"source":["classifier.model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BartForSequenceClassification(\n","  (model): BartModel(\n","    (shared): Embedding(50265, 1024, padding_idx=1)\n","    (encoder): BartEncoder(\n","      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n","      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n","      (layers): ModuleList(\n","        (0): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (6): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (7): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (8): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (9): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (10): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (11): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): BartDecoder(\n","      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n","      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n","      (layers): ModuleList(\n","        (0): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (6): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (7): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (8): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (9): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (10): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (11): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (classification_head): BartClassificationHead(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): Dropout(p=0.0, inplace=False)\n","    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"_A56QEE1dzIh"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"7wsd_Al93dA9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e80a3f84-09a5-41a6-b845-3e3d72b68f24"},"source":["for epoch in range(epochs):\n","    classifier.model.train()\n","    losses = 0.\n","    num_trained_seq = 0\n","    start_time = time.time()\n","    \n","    for batch_idx, batch in enumerate(train_data_loader):\n","        optimizer.zero_grad()\n","\n","        inputs = batch['inputs']\n","        label = batch['label']\n","\n","        outputs = classifier.model(**inputs)\n","        loss = criterion(outputs.logits, label)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        current_step = epoch*num_steps_per_epoch + (batch_idx + 1)\n","        writer.add_scalar(\"Loss/train\", loss, current_step)\n","\n","        if current_step % store_interval == 0:\n","            checkpoint_full_path = os.path.join(checkpoint_path, f\"checkpoint_{current_step}.bin\")\n","            classifier.model.save_pretrained(checkpoint_full_path)\n","\n","        current_batch_size = len(batch)\n","        num_trained_seq += current_batch_size\n","        losses += current_batch_size*loss.item()\n","        \n","        if (batch_idx + 1) % log_interval == 0:\n","            current_loss = losses / num_trained_seq\n","            elapsed = time.time() - start_time\n","            print('epoch: {:3d} | step: {:5d} | batch: {:5d} | lr: {:5.6f} | ms/batch: {:5.2f} | loss: {:5.3f}'.format(\n","                epoch, \n","                current_step,\n","                (batch_idx + 1),\n","                optimizer.param_groups[0]['lr'],\n","                elapsed * 1000 / log_interval,\n","                current_loss\n","            ))\n","\n","            losses = 0.\n","            num_trained_seq = 0\n","            start_time = time.time()\n","\n","        scheduler.step()\n","\n","writer.flush()\n","writer.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch:   0 | step:    32 | batch:    32 | lr: 0.000000 | ms/batch: 1044.04 | loss: 3.116\n","epoch:   0 | step:    64 | batch:    64 | lr: 0.000001 | ms/batch: 1049.02 | loss: 1.399\n","epoch:   0 | step:    96 | batch:    96 | lr: 0.000001 | ms/batch: 1023.86 | loss: 0.565\n","epoch:   0 | step:   128 | batch:   128 | lr: 0.000002 | ms/batch: 1037.67 | loss: 0.982\n","epoch:   0 | step:   160 | batch:   160 | lr: 0.000002 | ms/batch: 1034.52 | loss: 0.703\n","epoch:   0 | step:   192 | batch:   192 | lr: 0.000003 | ms/batch: 1029.82 | loss: 0.762\n","epoch:   0 | step:   224 | batch:   224 | lr: 0.000003 | ms/batch: 1032.81 | loss: 0.808\n","epoch:   0 | step:   256 | batch:   256 | lr: 0.000004 | ms/batch: 1035.05 | loss: 0.743\n","epoch:   0 | step:   288 | batch:   288 | lr: 0.000004 | ms/batch: 1033.07 | loss: 0.898\n","epoch:   0 | step:   320 | batch:   320 | lr: 0.000004 | ms/batch: 1034.50 | loss: 0.763\n","epoch:   0 | step:   352 | batch:   352 | lr: 0.000005 | ms/batch: 1034.14 | loss: 0.721\n","epoch:   0 | step:   384 | batch:   384 | lr: 0.000005 | ms/batch: 1495.75 | loss: 0.746\n","epoch:   0 | step:   416 | batch:   416 | lr: 0.000006 | ms/batch: 1061.24 | loss: 0.761\n","epoch:   0 | step:   448 | batch:   448 | lr: 0.000006 | ms/batch: 1022.46 | loss: 0.685\n","epoch:   0 | step:   480 | batch:   480 | lr: 0.000007 | ms/batch: 1031.61 | loss: 0.628\n","epoch:   0 | step:   512 | batch:   512 | lr: 0.000007 | ms/batch: 1037.06 | loss: 0.706\n","epoch:   0 | step:   544 | batch:   544 | lr: 0.000008 | ms/batch: 1032.26 | loss: 0.732\n","epoch:   0 | step:   576 | batch:   576 | lr: 0.000008 | ms/batch: 1033.06 | loss: 0.640\n","epoch:   0 | step:   608 | batch:   608 | lr: 0.000008 | ms/batch: 1035.28 | loss: 0.643\n","epoch:   0 | step:   640 | batch:   640 | lr: 0.000009 | ms/batch: 1035.44 | loss: 0.675\n","epoch:   0 | step:   672 | batch:   672 | lr: 0.000009 | ms/batch: 1034.40 | loss: 0.622\n","epoch:   0 | step:   704 | batch:   704 | lr: 0.000010 | ms/batch: 1031.51 | loss: 0.715\n","epoch:   1 | step:   753 | batch:    32 | lr: 0.000010 | ms/batch: 1487.48 | loss: 0.547\n","epoch:   1 | step:   785 | batch:    64 | lr: 0.000010 | ms/batch: 1048.83 | loss: 0.574\n","epoch:   1 | step:   817 | batch:    96 | lr: 0.000010 | ms/batch: 1025.19 | loss: 0.460\n","epoch:   1 | step:   849 | batch:   128 | lr: 0.000010 | ms/batch: 1038.36 | loss: 0.450\n","epoch:   1 | step:   881 | batch:   160 | lr: 0.000010 | ms/batch: 1034.85 | loss: 0.634\n","epoch:   1 | step:   913 | batch:   192 | lr: 0.000010 | ms/batch: 1033.63 | loss: 0.584\n","epoch:   1 | step:   945 | batch:   224 | lr: 0.000010 | ms/batch: 1036.10 | loss: 0.715\n","epoch:   1 | step:   977 | batch:   256 | lr: 0.000010 | ms/batch: 1034.36 | loss: 0.647\n","epoch:   1 | step:  1009 | batch:   288 | lr: 0.000010 | ms/batch: 1034.61 | loss: 0.539\n","epoch:   1 | step:  1041 | batch:   320 | lr: 0.000010 | ms/batch: 1034.63 | loss: 0.361\n","epoch:   1 | step:  1073 | batch:   352 | lr: 0.000010 | ms/batch: 1034.97 | loss: 0.586\n","epoch:   1 | step:  1105 | batch:   384 | lr: 0.000010 | ms/batch: 1459.27 | loss: 0.601\n","epoch:   1 | step:  1137 | batch:   416 | lr: 0.000010 | ms/batch: 1056.31 | loss: 0.600\n","epoch:   1 | step:  1169 | batch:   448 | lr: 0.000010 | ms/batch: 1027.77 | loss: 0.481\n","epoch:   1 | step:  1201 | batch:   480 | lr: 0.000010 | ms/batch: 1035.58 | loss: 0.454\n","epoch:   1 | step:  1233 | batch:   512 | lr: 0.000010 | ms/batch: 1035.49 | loss: 0.436\n","epoch:   1 | step:  1265 | batch:   544 | lr: 0.000010 | ms/batch: 1033.94 | loss: 0.633\n","epoch:   1 | step:  1297 | batch:   576 | lr: 0.000010 | ms/batch: 1038.68 | loss: 0.463\n","epoch:   1 | step:  1329 | batch:   608 | lr: 0.000010 | ms/batch: 1034.37 | loss: 0.762\n","epoch:   1 | step:  1361 | batch:   640 | lr: 0.000010 | ms/batch: 1034.08 | loss: 0.691\n","epoch:   1 | step:  1393 | batch:   672 | lr: 0.000010 | ms/batch: 1035.31 | loss: 0.578\n","epoch:   1 | step:  1425 | batch:   704 | lr: 0.000010 | ms/batch: 1035.12 | loss: 0.560\n","epoch:   2 | step:  1474 | batch:    32 | lr: 0.000010 | ms/batch: 1429.99 | loss: 0.506\n","epoch:   2 | step:  1506 | batch:    64 | lr: 0.000010 | ms/batch: 1047.50 | loss: 0.286\n","epoch:   2 | step:  1538 | batch:    96 | lr: 0.000010 | ms/batch: 1025.90 | loss: 0.259\n","epoch:   2 | step:  1570 | batch:   128 | lr: 0.000010 | ms/batch: 1037.61 | loss: 0.276\n","epoch:   2 | step:  1602 | batch:   160 | lr: 0.000010 | ms/batch: 1034.81 | loss: 0.358\n","epoch:   2 | step:  1634 | batch:   192 | lr: 0.000010 | ms/batch: 1033.42 | loss: 0.356\n","epoch:   2 | step:  1666 | batch:   224 | lr: 0.000010 | ms/batch: 1032.18 | loss: 0.530\n","epoch:   2 | step:  1698 | batch:   256 | lr: 0.000010 | ms/batch: 1037.10 | loss: 0.376\n","epoch:   2 | step:  1730 | batch:   288 | lr: 0.000010 | ms/batch: 1037.98 | loss: 0.429\n","epoch:   2 | step:  1762 | batch:   320 | lr: 0.000010 | ms/batch: 1033.60 | loss: 0.191\n","epoch:   2 | step:  1794 | batch:   352 | lr: 0.000010 | ms/batch: 1034.63 | loss: 0.504\n","epoch:   2 | step:  1826 | batch:   384 | lr: 0.000010 | ms/batch: 1345.42 | loss: 0.449\n","epoch:   2 | step:  1858 | batch:   416 | lr: 0.000010 | ms/batch: 1051.48 | loss: 0.331\n","epoch:   2 | step:  1890 | batch:   448 | lr: 0.000010 | ms/batch: 1030.22 | loss: 0.346\n","epoch:   2 | step:  1922 | batch:   480 | lr: 0.000010 | ms/batch: 1035.02 | loss: 0.374\n","epoch:   2 | step:  1954 | batch:   512 | lr: 0.000010 | ms/batch: 1037.69 | loss: 0.372\n","epoch:   2 | step:  1986 | batch:   544 | lr: 0.000010 | ms/batch: 1037.41 | loss: 0.374\n","epoch:   2 | step:  2018 | batch:   576 | lr: 0.000010 | ms/batch: 1034.59 | loss: 0.497\n","epoch:   2 | step:  2050 | batch:   608 | lr: 0.000010 | ms/batch: 1033.52 | loss: 0.444\n","epoch:   2 | step:  2082 | batch:   640 | lr: 0.000010 | ms/batch: 1034.58 | loss: 0.558\n","epoch:   2 | step:  2114 | batch:   672 | lr: 0.000010 | ms/batch: 1033.40 | loss: 0.497\n","epoch:   2 | step:  2146 | batch:   704 | lr: 0.000010 | ms/batch: 1035.75 | loss: 0.283\n","epoch:   3 | step:  2195 | batch:    32 | lr: 0.000010 | ms/batch: 1320.99 | loss: 0.182\n","epoch:   3 | step:  2227 | batch:    64 | lr: 0.000010 | ms/batch: 1039.98 | loss: 0.196\n","epoch:   3 | step:  2259 | batch:    96 | lr: 0.000010 | ms/batch: 1031.99 | loss: 0.381\n","epoch:   3 | step:  2291 | batch:   128 | lr: 0.000010 | ms/batch: 1038.72 | loss: 0.149\n","epoch:   3 | step:  2323 | batch:   160 | lr: 0.000010 | ms/batch: 1036.92 | loss: 0.109\n","epoch:   3 | step:  2355 | batch:   192 | lr: 0.000010 | ms/batch: 1036.14 | loss: 0.430\n","epoch:   3 | step:  2387 | batch:   224 | lr: 0.000010 | ms/batch: 1033.53 | loss: 0.334\n","epoch:   3 | step:  2419 | batch:   256 | lr: 0.000010 | ms/batch: 1034.29 | loss: 0.358\n","epoch:   3 | step:  2451 | batch:   288 | lr: 0.000010 | ms/batch: 1034.74 | loss: 0.437\n","epoch:   3 | step:  2483 | batch:   320 | lr: 0.000010 | ms/batch: 1034.28 | loss: 0.503\n","epoch:   3 | step:  2515 | batch:   352 | lr: 0.000009 | ms/batch: 1034.93 | loss: 0.298\n","epoch:   3 | step:  2547 | batch:   384 | lr: 0.000009 | ms/batch: 1482.50 | loss: 0.474\n","epoch:   3 | step:  2579 | batch:   416 | lr: 0.000009 | ms/batch: 1059.68 | loss: 0.241\n","epoch:   3 | step:  2611 | batch:   448 | lr: 0.000009 | ms/batch: 1026.93 | loss: 0.077\n","epoch:   3 | step:  2643 | batch:   480 | lr: 0.000009 | ms/batch: 1035.03 | loss: 0.313\n","epoch:   3 | step:  2675 | batch:   512 | lr: 0.000009 | ms/batch: 1037.36 | loss: 0.305\n","epoch:   3 | step:  2707 | batch:   544 | lr: 0.000009 | ms/batch: 1033.02 | loss: 0.068\n","epoch:   3 | step:  2739 | batch:   576 | lr: 0.000009 | ms/batch: 1034.78 | loss: 0.116\n","epoch:   3 | step:  2771 | batch:   608 | lr: 0.000009 | ms/batch: 1036.13 | loss: 0.305\n","epoch:   3 | step:  2803 | batch:   640 | lr: 0.000009 | ms/batch: 1035.49 | loss: 0.128\n","epoch:   3 | step:  2835 | batch:   672 | lr: 0.000009 | ms/batch: 1034.37 | loss: 0.307\n","epoch:   3 | step:  2867 | batch:   704 | lr: 0.000009 | ms/batch: 1034.47 | loss: 0.249\n","epoch:   4 | step:  2916 | batch:    32 | lr: 0.000009 | ms/batch: 1447.55 | loss: 0.053\n","epoch:   4 | step:  2948 | batch:    64 | lr: 0.000009 | ms/batch: 1051.96 | loss: 0.340\n","epoch:   4 | step:  2980 | batch:    96 | lr: 0.000009 | ms/batch: 1027.55 | loss: 0.075\n","epoch:   4 | step:  3012 | batch:   128 | lr: 0.000009 | ms/batch: 1039.01 | loss: 0.189\n","epoch:   4 | step:  3044 | batch:   160 | lr: 0.000009 | ms/batch: 1037.58 | loss: 0.024\n","epoch:   4 | step:  3076 | batch:   192 | lr: 0.000009 | ms/batch: 1033.85 | loss: 0.160\n","epoch:   4 | step:  3108 | batch:   224 | lr: 0.000009 | ms/batch: 1033.90 | loss: 0.250\n","epoch:   4 | step:  3140 | batch:   256 | lr: 0.000009 | ms/batch: 1034.37 | loss: 0.102\n","epoch:   4 | step:  3172 | batch:   288 | lr: 0.000009 | ms/batch: 1035.18 | loss: 0.028\n","epoch:   4 | step:  3204 | batch:   320 | lr: 0.000009 | ms/batch: 1034.39 | loss: 0.078\n","epoch:   4 | step:  3236 | batch:   352 | lr: 0.000009 | ms/batch: 1034.49 | loss: 0.006\n","epoch:   4 | step:  3268 | batch:   384 | lr: 0.000009 | ms/batch: 1325.04 | loss: 0.257\n","epoch:   4 | step:  3300 | batch:   416 | lr: 0.000009 | ms/batch: 1052.82 | loss: 0.128\n","epoch:   4 | step:  3332 | batch:   448 | lr: 0.000009 | ms/batch: 1031.14 | loss: 0.138\n","epoch:   4 | step:  3364 | batch:   480 | lr: 0.000009 | ms/batch: 1034.07 | loss: 0.021\n","epoch:   4 | step:  3396 | batch:   512 | lr: 0.000009 | ms/batch: 1037.04 | loss: 0.023\n","epoch:   4 | step:  3428 | batch:   544 | lr: 0.000009 | ms/batch: 1036.52 | loss: 0.232\n","epoch:   4 | step:  3460 | batch:   576 | lr: 0.000009 | ms/batch: 1035.04 | loss: 0.112\n","epoch:   4 | step:  3492 | batch:   608 | lr: 0.000009 | ms/batch: 1032.69 | loss: 0.030\n","epoch:   4 | step:  3524 | batch:   640 | lr: 0.000009 | ms/batch: 1033.84 | loss: 0.012\n","epoch:   4 | step:  3556 | batch:   672 | lr: 0.000009 | ms/batch: 1033.86 | loss: 0.133\n","epoch:   4 | step:  3588 | batch:   704 | lr: 0.000009 | ms/batch: 1033.61 | loss: 0.178\n","epoch:   5 | step:  3637 | batch:    32 | lr: 0.000009 | ms/batch: 1485.17 | loss: 0.075\n","epoch:   5 | step:  3669 | batch:    64 | lr: 0.000009 | ms/batch: 1054.76 | loss: 0.221\n","epoch:   5 | step:  3701 | batch:    96 | lr: 0.000009 | ms/batch: 1025.41 | loss: 0.091\n","epoch:   5 | step:  3733 | batch:   128 | lr: 0.000009 | ms/batch: 1033.88 | loss: 0.277\n","epoch:   5 | step:  3765 | batch:   160 | lr: 0.000009 | ms/batch: 1036.04 | loss: 0.140\n","epoch:   5 | step:  3797 | batch:   192 | lr: 0.000009 | ms/batch: 1033.25 | loss: 0.050\n","epoch:   5 | step:  3829 | batch:   224 | lr: 0.000009 | ms/batch: 1034.00 | loss: 0.012\n","epoch:   5 | step:  3861 | batch:   256 | lr: 0.000009 | ms/batch: 1037.66 | loss: 0.006\n","epoch:   5 | step:  3893 | batch:   288 | lr: 0.000009 | ms/batch: 1036.38 | loss: 0.136\n","epoch:   5 | step:  3925 | batch:   320 | lr: 0.000009 | ms/batch: 1033.83 | loss: 0.026\n","epoch:   5 | step:  3957 | batch:   352 | lr: 0.000009 | ms/batch: 1035.52 | loss: 0.069\n","epoch:   5 | step:  3989 | batch:   384 | lr: 0.000009 | ms/batch: 1438.32 | loss: 0.086\n","epoch:   5 | step:  4021 | batch:   416 | lr: 0.000009 | ms/batch: 1061.18 | loss: 0.158\n","epoch:   5 | step:  4053 | batch:   448 | lr: 0.000009 | ms/batch: 1031.10 | loss: 0.350\n","epoch:   5 | step:  4085 | batch:   480 | lr: 0.000009 | ms/batch: 1034.02 | loss: 0.018\n","epoch:   5 | step:  4117 | batch:   512 | lr: 0.000009 | ms/batch: 1039.90 | loss: 0.206\n","epoch:   5 | step:  4149 | batch:   544 | lr: 0.000009 | ms/batch: 1034.71 | loss: 0.054\n","epoch:   5 | step:  4181 | batch:   576 | lr: 0.000009 | ms/batch: 1034.72 | loss: 0.161\n","epoch:   5 | step:  4213 | batch:   608 | lr: 0.000009 | ms/batch: 1033.95 | loss: 0.035\n","epoch:   5 | step:  4245 | batch:   640 | lr: 0.000009 | ms/batch: 1035.22 | loss: 0.118\n","epoch:   5 | step:  4277 | batch:   672 | lr: 0.000009 | ms/batch: 1035.05 | loss: 0.335\n","epoch:   5 | step:  4309 | batch:   704 | lr: 0.000009 | ms/batch: 1034.70 | loss: 0.127\n","epoch:   6 | step:  4358 | batch:    32 | lr: 0.000009 | ms/batch: 1444.63 | loss: 0.287\n","epoch:   6 | step:  4390 | batch:    64 | lr: 0.000009 | ms/batch: 1055.07 | loss: 0.049\n","epoch:   6 | step:  4422 | batch:    96 | lr: 0.000009 | ms/batch: 1028.20 | loss: 0.121\n","epoch:   6 | step:  4454 | batch:   128 | lr: 0.000009 | ms/batch: 1037.43 | loss: 0.136\n","epoch:   6 | step:  4486 | batch:   160 | lr: 0.000009 | ms/batch: 1036.08 | loss: 0.041\n","epoch:   6 | step:  4518 | batch:   192 | lr: 0.000009 | ms/batch: 1033.38 | loss: 0.044\n","epoch:   6 | step:  4550 | batch:   224 | lr: 0.000009 | ms/batch: 1038.08 | loss: 0.262\n","epoch:   6 | step:  4582 | batch:   256 | lr: 0.000009 | ms/batch: 1033.52 | loss: 0.134\n","epoch:   6 | step:  4614 | batch:   288 | lr: 0.000009 | ms/batch: 1033.75 | loss: 0.046\n","epoch:   6 | step:  4646 | batch:   320 | lr: 0.000009 | ms/batch: 1038.45 | loss: 0.111\n","epoch:   6 | step:  4678 | batch:   352 | lr: 0.000009 | ms/batch: 1034.29 | loss: 0.048\n","epoch:   6 | step:  4710 | batch:   384 | lr: 0.000009 | ms/batch: 1307.97 | loss: 0.400\n","epoch:   6 | step:  4742 | batch:   416 | lr: 0.000009 | ms/batch: 1051.67 | loss: 0.056\n","epoch:   6 | step:  4774 | batch:   448 | lr: 0.000009 | ms/batch: 1031.72 | loss: 0.011\n","epoch:   6 | step:  4806 | batch:   480 | lr: 0.000009 | ms/batch: 1033.78 | loss: 0.092\n","epoch:   6 | step:  4838 | batch:   512 | lr: 0.000009 | ms/batch: 1038.55 | loss: 0.050\n","epoch:   6 | step:  4870 | batch:   544 | lr: 0.000009 | ms/batch: 1035.52 | loss: 0.205\n","epoch:   6 | step:  4902 | batch:   576 | lr: 0.000009 | ms/batch: 1034.02 | loss: 0.199\n","epoch:   6 | step:  4934 | batch:   608 | lr: 0.000009 | ms/batch: 1035.23 | loss: 0.111\n","epoch:   6 | step:  4966 | batch:   640 | lr: 0.000009 | ms/batch: 1035.16 | loss: 0.084\n","epoch:   6 | step:  4998 | batch:   672 | lr: 0.000009 | ms/batch: 1035.58 | loss: 0.237\n"],"name":"stdout"}]}]}